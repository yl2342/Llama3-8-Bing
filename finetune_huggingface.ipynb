{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6495e840-ad14-4223-bb71-399aacc935b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## library set up for grace ondemand jupyternotebook\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ctypes\n",
    "import os\n",
    "ctypes.CDLL('/lib64/libsnappy.so.1')\n",
    "import pyarrow\n",
    "from datasets import Dataset\n",
    "from collections import Counter\n",
    "import random\n",
    "import evaluate\n",
    "random.seed(666)\n",
    "from transformers import BertTokenizer, BertModel, AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "# baseline_only only trains the classifier head added after the last hidden layer output of the transformer block\n",
    "baseline_only = True\n",
    "\n",
    "# model_name has to be \"BERT\" or \"Llama2-7B\"\n",
    "model_name = \"Llama2-7B\"\n",
    "\n",
    "print(\"Libraries load finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932e841f-2f67-46e7-858d-2b02990c3c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code block modifies the NLP model structure for classification task and input the models and tokenizers\n",
    "\n",
    "class bertClassifier(nn.Module):\n",
    "    def __init__(self, model, num_labels=2,only_classifier = False):\n",
    "        super(bertClassifier, self).__init__()\n",
    "        self.only_classifier = only_classifier\n",
    "        if only_classifier:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.bert = model\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, input_ids,labels, attention_mask=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        bos_output = outputs.pooler_output.requires_grad_(not self.only_classifier)\n",
    "        logits = self.classifier(bos_output.bfloat16())\n",
    "        new_output = {\"loss\" : self.loss_fn(logits, labels),\n",
    "                     \"logits\" : logits}\n",
    "        return new_output\n",
    "\n",
    "\n",
    "class LlamaClassifier(nn.Module):\n",
    "    def __init__(self, model, num_labels=2 , only_classifier=False):\n",
    "        super(LlamaClassifier, self).__init__()\n",
    "        self.only_classifier = only_classifier  \n",
    "        if only_classifier:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            lora_config = LoraConfig(task_type=\"SEQ_CLS\", r=16, lora_alpha=4, lora_dropout=0.1)\n",
    "            model = get_peft_model(model, lora_config)\n",
    "            model.print_trainable_parameters()\n",
    "        self.llama = model\n",
    "        self.classifier = nn.Linear(self.llama.config.hidden_size, num_labels)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, labels, attention_mask=None):\n",
    "        outputs = self.llama(input_ids, attention_mask=attention_mask)\n",
    "        index_c = torch.sum(attention_mask, dim=1) - 1\n",
    "        bos_output1 = outputs.hidden_states[-1]\n",
    "        bos_output1 = bos_output1.requires_grad_(not self.only_classifier)\n",
    "        bos_output = torch.stack([bos_output1[i, idx] for i, idx in enumerate(index_c)])\n",
    "        logits = self.classifier(bos_output.bfloat16())\n",
    "        new_output = {\"loss\": self.loss_fn(logits, labels), \"logits\": logits}\n",
    "        return new_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab182219-b640-4fb0-8857-fd6d19ea2f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer_and_model(model_name, baseline_only):\n",
    "    if model_name == \"BERT\":\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "        model = BertModel.from_pretrained(\"bert-base-cased\",device_map = 'cuda')\n",
    "        model = bertClassifier(num_labels=2,only_classifier = baseline_only,model=model)\n",
    "        model = model.bfloat16()\n",
    "    elif model_name == \"Llama2-7B\":\n",
    "        from peft import get_peft_model, LoraConfig\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/llama-2-7b-chat-hf\",padding_side='right')\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"NousResearch/llama-2-7b-chat-hf\",device_map = 'cuda')\n",
    "        model.config.use_cache = False\n",
    "        model.config.output_hidden_states = True\n",
    "        model = LlamaClassifier(num_labels=2,only_classifier = baseline_only,model=model)\n",
    "        model = model.bfloat16()\n",
    "    else:\n",
    "        raise ValueError('In this project we only finetuned \"BERT\" or \"Llama2-7B\" models')\n",
    "    print(\"load \"+model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "model, tokenizer = load_tokenizer_and_model(model_name, baseline_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3276c19-c52e-44bc-9ba0-aeccfa427a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code block directly reads training and testing dataset from the github data source\n",
    "\n",
    "twitter_training = pd.read_json('https://raw.githubusercontent.com/EducationalTestingService/sarcasm/master/twitter/sarcasm_detection_shared_task_twitter_training.jsonl',lines=True)\n",
    "twitter_testing = pd.read_json('https://raw.githubusercontent.com/EducationalTestingService/sarcasm/master/twitter/sarcasm_detection_shared_task_twitter_testing.jsonl',lines=True)\n",
    "reddit_training = pd.read_json('https://raw.githubusercontent.com/EducationalTestingService/sarcasm/master/reddit/sarcasm_detection_shared_task_reddit_training.jsonl',lines=True)\n",
    "reddit_testing = pd.read_json('https://raw.githubusercontent.com/EducationalTestingService/sarcasm/master/reddit/sarcasm_detection_shared_task_reddit_testing.jsonl',lines=True)\n",
    "combined_training, combined_testing  = pd.concat([reddit_training, twitter_training], ignore_index=True), pd.concat([reddit_testing, twitter_testing], ignore_index=True)\n",
    "\n",
    "def prepare_df(df):\n",
    "    labels = {'NOT_SARCASM': 0, 'SARCASM': 1}\n",
    "    df['label'] = df['label'].map(labels)\n",
    "    df['full_seq'] = \"context: \" + df['context'].astype(str) + \" response: \" + df['response'].astype(str)\n",
    "    df.drop(columns=['context', 'response'], inplace=True)\n",
    "    df = df.to_dict(orient='list')\n",
    "    df = Dataset.from_dict(df)\n",
    "    encoded_input = tokenizer.batch_encode_plus( df[\"full_seq\"], return_tensors='pt', padding=True, truncation=True,max_length=512)\n",
    "    input_ids = encoded_input['input_ids']\n",
    "    attention_masks = encoded_input['attention_mask']\n",
    "    df = {\"input_ids\" : encoded_input[\"input_ids\"], \"attention_mask\" : encoded_input[\"attention_mask\"], \"labels\" : torch.tensor(df[\"label\"])}  \n",
    "    df = Dataset.from_dict(df)\n",
    "    return df\n",
    "\n",
    "combined_training, combined_testing = prepare_df(combined_training), prepare_df(combined_testing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5b0d65-c13d-45f5-9f99-6f0f29ea97b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code block initializes training set up and hyperparameters\n",
    "\n",
    "if model_name = \"BERT\":\n",
    "    lr = 1e-3 if baseline_only else 2e-5\n",
    "else:\n",
    "    lr = 1e-4 if baseline_only else 5e-5\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=10,              # number of training epochs\n",
    "    optim = \"paged_adamw_32bit\",\n",
    "    per_device_train_batch_size=5,  # batch size for training\n",
    "    warmup_steps=100,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',           # directory for storing logs\n",
    "    evaluation_strategy='steps',\n",
    "    learning_rate=lr,\n",
    "    max_grad_norm = 0.5, \n",
    "    gradient_accumulation_steps = 5, \n",
    "    logging_steps=50)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=combined_training,  \n",
    "    eval_dataset=combined_testing, \n",
    "    compute_metrics=compute_metrics,)\n",
    "print(\"start training\")\n",
    "trainer.train()\n",
    "torch.save({'model': model1.state_dict()}, model_name+'_'+baseline_only+'_.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e726347-7968-4012-a040-a39e38b034fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "hist = trainer.state.log_history\n",
    "k = int((len(hist)-1)/2)\n",
    "train_loss = [hist[2*t][\"loss\"] for t in range(1,k)]\n",
    "eval_loss = [hist[2*t+1][\"eval_loss\"] for t in range(1,k)]\n",
    "eval_accuracy = [hist[2*t+1][\"eval_accuracy\"] for t in range(1,k)]\n",
    "\n",
    "plt.figure(figsize=(16, 5))\n",
    "# Subplot 1: Loss\n",
    "plt.subplot(1, 2, 1)  # 1 row, 2 columns, first subplot\n",
    "plt.plot(train_loss, label='Training Loss', color='blue')\n",
    "plt.plot(eval_loss, label='Eval/Test Loss', color='red')\n",
    "plt.title(model_name+'_'+baseline_only+'_Training and Eval/Test Loss')\n",
    "plt.xlabel('Stages')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(0.4, 0.8) \n",
    "\n",
    "# Subplot 2: Accuracy\n",
    "plt.subplot(1, 2, 2)  # 1 row, 2 columns, second subplot\n",
    "plt.plot(eval_accuracy, label='Eval/Test Accuracy', color='green')\n",
    "plt.title(model_name+'_'+baseline_only+'_Eval/Test Accuracy')\n",
    "plt.xlabel('Stages')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(0.5, 0.8) \n",
    "\n",
    "plt.savefig(model_name+'_training_error_'+baseline_only+'.png', dpi=500) \n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
