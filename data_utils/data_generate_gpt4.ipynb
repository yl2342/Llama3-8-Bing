{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (1.23.3)\n",
      "Requirement already satisfied: wandb in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (0.16.6)\n",
      "Requirement already satisfied: python-dotenv in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: datasets in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (2.19.0)\n",
      "Requirement already satisfied: huggingface_hub in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (0.22.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from openai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from openai) (2.7.0)\n",
      "Requirement already satisfied: sniffio in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from wandb) (5.9.8)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from wandb) (1.45.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: setproctitle in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: filelock in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from datasets) (3.13.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from datasets) (16.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: packaging in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: certifi in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.18.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/yuntian/opt/anaconda3/envs/cpsc577/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade openai wandb python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import wandb\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## System prompt\n",
    "\n",
    "You are a helpful AI assistant who is an expert in language art. You are going to help me with a task described as follow: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User prompt\n",
    "\n",
    "You are a helpful AI assistant who is an expert in language art. You are going to help me with a task described as follow: \n",
    "\n",
    "I'm planning to fine-tune an open-source, chat-based large language model to enhance its understanding of sarcasm, specifically to mimic Chandler Bing from the TV sitcom Friends, known for his sarcastic wit. The method involves using direct preference optimization and Reinforcement Learning from Human Feedback (RLHF). I need your assistance in creating a preference dataset for this task based on the following guidelines:\n",
    "\n",
    "Source Material: Utilize the screenplay from an episode of Friends that I'll provide you later below.\n",
    "Task Details:\n",
    "1. Extraction: Identify and extract every line spoken by Chandler Bing.\n",
    "2. Context Summarization: For each line extracted, provide a brief but useful summary of the surrounding context. If Chandler's line is a response to another character, append that information to the end of the context summary. If Chandler's line is a standalone joke, just provide a brief summary of the scene or context.\n",
    "3. Sarcasm Analysis: Determine whether each quote is sarcastic. Support your classification with a brief reason.\n",
    "4. Response Generation: For each quote, generate two brief responses that Chandler Bing is unlikely to say. One response should be sarcastic, and the other should be sincere. Remember both responses should neither fit Chandler Bing's and nor sound like Chandler Bing's original quote at all. These will serve as the rejected response in the dataset, contrasting with Chandler's original line which is the preferred response.\n",
    "5. Output Format: Organize the results into a python-dictionary format with six keys. The final output should be a string that can be evaluated and converted into a real python dictionary and json file later. Please ensure that you skip any description or explanation of the analysis process and your output should be structured exactly as follows:\n",
    "{\n",
    "    \"Chandler_quote\": Original line from Chandler Bing,\n",
    "    \"Context\": Brief summary of the context around the quote,\n",
    "    \"Sarcasm\": True/False,\n",
    "    \"Reason\": Explanation for the sarcasm classification,\n",
    "    \"Unlike_chandler_sarcastic\": Hypothetical sarcastic response unlikely to be said by Chandler Bing,\n",
    "    \"Unlike_chandler_sincere\": Hypothetical sincere response unlikely to be said by Chandler Bing\n",
    "}\n",
    "\n",
    "Below is the screenplay from the episode of Friends that you should use for this task. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the system_prompt.txt and user_prompt.txt from the file in the current directory\n",
    "with open(\"system_prompt.txt\", \"r\") as f:\n",
    "    SYSTEM_PROMPT = f.read()   \n",
    "\n",
    "with open(\"user_prompt.txt\", \"r\") as f:\n",
    "    USER_PROMPT = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the frieds_screenplay.jsonl file and load the data\n",
    "friends_screenplay= []\n",
    "\n",
    "with open(\"friends_screenplay.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        friends_screenplay.append(json.loads(line))\n",
    "\n",
    "# split the friends_screenplat_10seaons by season information extracted frmo id field, eg. S01E01\n",
    "friends_screenplay_by_season = {}\n",
    "for episode in friends_screenplay:\n",
    "    season = episode[\"id\"][:3]\n",
    "    if season not in friends_screenplay_by_season:\n",
    "        friends_screenplay_by_season[season] = []\n",
    "    friends_screenplay_by_season[season].append(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a subset of the friends_screenplay_by_season with only 2 episodes from season 1\n",
    "# # for testing/debugging purposes \n",
    "# friends_screenplay_by_season_subset =  friends_screenplay_by_season.copy()\n",
    "# friends_screenplay_by_season_subset['S01'] = friends_screenplay_by_season['S01'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the .env file for the OPENAI_API_KEY\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "def gpt_generate_data(client: OpenAI, model: str, screenplay_byseason: dict, season: str, \n",
    "                  user_prompt: str , system_prompt: str, \n",
    "                  max_tokens: int = 4096, temperature: int = 0, frequency_penalty: float = 0.0):\n",
    "    \"\"\"\n",
    "    iterate and generate the required preference data for the specified season of friends\n",
    "\n",
    "    Args:\n",
    "        client: OpenAI client instance\n",
    "        model: the model to use, here use gpt-4-turbo / gpt-4-turbo\n",
    "        screenplay_byseason: the screen play data by season \n",
    "        season: the specified season \n",
    "        user_prompt: the user prompt\n",
    "        system_prompt: the system prompt\n",
    "        max_tokens: the maximum tokens to use\n",
    "        temperature: the temperature to use\n",
    "        frequency_penalty: the frequency penalty to use\n",
    "    \"\"\"\n",
    "    generated_result = list(dict())\n",
    "\n",
    "    for episode in tqdm(screenplay_byseason[season]):\n",
    "        id = episode['id']\n",
    "        screenplay = episode['screenplay'] \n",
    "        user_prompt_with_screenplay = user_prompt + screenplay\n",
    "\n",
    "        prompt_messages =[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt_with_screenplay}\n",
    "        ]\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model= model,\n",
    "            messages = prompt_messages,\n",
    "            temperature= temperature, # 0 is no randomness,a lower temperature (closer to 0) makes the model's responses more deterministic and conservative.\n",
    "            max_tokens= max_tokens, # use the max tokens to caputre all results\n",
    "            frequency_penalty=frequency_penalty # 0.0 means no penalty, a higher frequency_penalty means the model will penalize new tokens based on their frequency in the training data.\n",
    "        )\n",
    "\n",
    "        output = response.choices[0].message.content\n",
    "\n",
    "        generated_result.append({\"id\": id, \"screenplay\": screenplay, \"generated_result\": output})\n",
    "    \n",
    "    return generated_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE THE COST\n",
    "* It takes ~30 minutes and ~$3 to generate the data for each season of Friends (~ 20 episodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Season 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 7.87 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [26:31<00:00, 66.33s/it]\n"
     ]
    }
   ],
   "source": [
    "# generate the data for season 1 using the gpt-4-turbo model\n",
    "s1_data_gpt4turbo = gpt_generate_data(client = client, model = \"gpt-4-turbo\", \n",
    "                                 screenplay_byseason = friends_screenplay_by_season, \n",
    "                                 season = \"S01\", user_prompt = USER_PROMPT, system_prompt = SYSTEM_PROMPT,\n",
    "                                 max_tokens = 4096, temperature = 0, frequency_penalty = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Season 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [27:01<00:00, 70.52s/it]\n"
     ]
    }
   ],
   "source": [
    "# generate the data for season 2 using the gpt-4-turbo model\n",
    "s2_data_gpt4turbo = gpt_generate_data(client = client, model = \"gpt-4-turbo\", \n",
    "                                 screenplay_byseason = friends_screenplay_by_season, \n",
    "                                 season = \"S02\", user_prompt = USER_PROMPT, system_prompt = SYSTEM_PROMPT,\n",
    "                                 max_tokens = 4096, temperature = 0, frequency_penalty = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Season 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [23:21<00:00, 56.07s/it]\n"
     ]
    }
   ],
   "source": [
    "# generate the data for season 3 using the gpt-4-turbo model\n",
    "s3_data_gpt4turbo = gpt_generate_data(client = client, model = \"gpt-4-turbo\", \n",
    "                                 screenplay_byseason = friends_screenplay_by_season, \n",
    "                                 season = \"S03\", user_prompt = USER_PROMPT, system_prompt = SYSTEM_PROMPT,\n",
    "                                 max_tokens = 4096, temperature = 0, frequency_penalty = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Season 4    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [25:18<00:00, 66.00s/it]\n"
     ]
    }
   ],
   "source": [
    "# generate the data for season 4 using the gpt-4-turbo model\n",
    "s4_data_gpt4turbo = gpt_generate_data(client = client, model = \"gpt-4-turbo\", \n",
    "                                 screenplay_byseason = friends_screenplay_by_season, \n",
    "                                 season = \"S04\", user_prompt = USER_PROMPT, system_prompt = SYSTEM_PROMPT,\n",
    "                                 max_tokens = 4096, temperature = 0, frequency_penalty = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Season 5    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [24:41<00:00, 64.41s/it] \n"
     ]
    }
   ],
   "source": [
    "# generate the data for season 5 using the gpt-4-turbo model\n",
    "s5_data_gpt4turbo = gpt_generate_data(client = client, model = \"gpt-4-turbo\", \n",
    "                                 screenplay_byseason = friends_screenplay_by_season, \n",
    "                                 season = \"S05\", user_prompt = USER_PROMPT, system_prompt = SYSTEM_PROMPT,\n",
    "                                 max_tokens = 4096, temperature = 0, frequency_penalty = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Season 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [22:53<00:00, 59.73s/it]\n"
     ]
    }
   ],
   "source": [
    "# generate the data for season 6 using the gpt-4-turbo model\n",
    "s6_data_gpt4turbo = gpt_generate_data(client = client, model = \"gpt-4-turbo\", \n",
    "                                 screenplay_byseason = friends_screenplay_by_season, \n",
    "                                 season = \"S06\", user_prompt = USER_PROMPT, system_prompt = SYSTEM_PROMPT,\n",
    "                                 max_tokens = 4096, temperature = 0, frequency_penalty = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Season 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [15:10<00:00, 37.96s/it]\n"
     ]
    }
   ],
   "source": [
    "# generate the data for season 7 using the gpt-4-turbo model\n",
    "s7_data_gpt4turbo = gpt_generate_data(client = client, model = \"gpt-4-turbo\", \n",
    "                                 screenplay_byseason = friends_screenplay_by_season, \n",
    "                                 season = \"S07\", user_prompt = USER_PROMPT, system_prompt = SYSTEM_PROMPT,\n",
    "                                 max_tokens = 4096, temperature = 0, frequency_penalty = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Season 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [18:38<00:00, 48.63s/it]\n"
     ]
    }
   ],
   "source": [
    "# generate the data for season 8 using the gpt-4-turbo model\n",
    "s8_data_gpt4turbo = gpt_generate_data(client = client, model = \"gpt-4-turbo\", \n",
    "                                 screenplay_byseason = friends_screenplay_by_season, \n",
    "                                 season = \"S08\", user_prompt = USER_PROMPT, system_prompt = SYSTEM_PROMPT,\n",
    "                                 max_tokens = 4096, temperature = 0, frequency_penalty = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Season 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [21:57<00:00, 57.27s/it]\n"
     ]
    }
   ],
   "source": [
    "# generate the data for season 9 using the gpt-4-turbo model\n",
    "s9_data_gpt4turbo = gpt_generate_data(client = client, model = \"gpt-4-turbo\", \n",
    "                                 screenplay_byseason = friends_screenplay_by_season, \n",
    "                                 season = \"S09\", user_prompt = USER_PROMPT, system_prompt = SYSTEM_PROMPT,\n",
    "                                 max_tokens = 4096, temperature = 0, frequency_penalty = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Season 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [16:57<00:00, 59.84s/it]\n"
     ]
    }
   ],
   "source": [
    "# generate the data for season 10 using the gpt-4-turbo model\n",
    "s10_data_gpt4turbo = gpt_generate_data(client = client, model = \"gpt-4-turbo\", \n",
    "                                 screenplay_byseason = friends_screenplay_by_season, \n",
    "                                 season = \"S10\", user_prompt = USER_PROMPT, system_prompt = SYSTEM_PROMPT,\n",
    "                                 max_tokens = 4096, temperature = 0, frequency_penalty = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combine all seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_generated_str(generted_data: list):\n",
    "    \"\"\"\n",
    "    process the generated data t\n",
    "\n",
    "    Args:\n",
    "        generted_data: the generated data list for specific season\n",
    "\n",
    "    Returns:\n",
    "        the processed data: lisf of dict\n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "    for episode in generted_data:\n",
    "        try:\n",
    "            output_string = episode['generated_result']\n",
    "            start = output_string.find('[')\n",
    "            end = output_string.rfind(']') # need to +1 to include the closing bracket\n",
    "\n",
    "            # Extract the content between these positions\n",
    "            data_per_episode = eval(output_string[start:end+1])\n",
    "            for data in data_per_episode:\n",
    "                data.update({\"episode\": episode['id']}) \n",
    "            processed_data += data_per_episode\n",
    "        except:\n",
    "            print(f\"Error processing episode {episode['id']}\")\n",
    "    return processed_data\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing episode S09E13\n"
     ]
    }
   ],
   "source": [
    "## concatenate the list of generated results for all seasons\n",
    "# write into the all_seasons_data_gpt4turbo.jsonl file\n",
    "all_seasons_data_gpt4turbo = process_generated_str(s1_data_gpt4turbo)+\\\n",
    "                            process_generated_str(s2_data_gpt4turbo)+\\\n",
    "                            process_generated_str(s3_data_gpt4turbo)+\\\n",
    "                            process_generated_str(s4_data_gpt4turbo)+\\\n",
    "                            process_generated_str(s5_data_gpt4turbo)+\\\n",
    "                            process_generated_str(s6_data_gpt4turbo)+\\\n",
    "                            process_generated_str(s7_data_gpt4turbo)+\\\n",
    "                            process_generated_str(s8_data_gpt4turbo)+\\\n",
    "                            process_generated_str(s9_data_gpt4turbo)+\\\n",
    "                            process_generated_str(s10_data_gpt4turbo)\n",
    "\n",
    "with open(\"chandler_bing_sarcasm_analysis_all_seasons.jsonl\", \"w\") as f:\n",
    "    for item in all_seasons_data_gpt4turbo:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## to read the all_seasons_data_gpt4turbo.jsonl file\n",
    "# all_seasons_data_gpt4turbo = []\n",
    "# with open(\"chandler_bing_sarcasm_analysis_all_seasons.jsonl\", \"r\") as f:\n",
    "#     for line in f:\n",
    "#         all_seasons_data_gpt4turbo.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sarcastic comments: 1162\n",
      "Total number of comments: 1971\n",
      "Percentage of sarcastic comments: 0.5895484525621512\n"
     ]
    }
   ],
   "source": [
    "sarcasm = [ data['Sarcasm'] for data in all_seasons_data_gpt4turbo]\n",
    "sarcasm_count = sum(sarcasm)\n",
    "total_count = len(sarcasm)\n",
    "sarcasm_percentage = sarcasm_count/total_count\n",
    "print(f\"Total number of sarcastic comments: {sarcasm_count}\")\n",
    "print(f\"Total number of comments: {total_count}\")\n",
    "print(f\"Percentage of sarcastic comments: {sarcasm_percentage}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
